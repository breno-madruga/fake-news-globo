{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the news of \"O Globo - Fato ou Fake\" related to the \"Fake News\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import re, csv, pandas as pd, numpy as np, time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data from its URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_webdriver(url, is_firefox=True):\n",
    "    # Choosing the webdriver.\n",
    "    if not is_firefox:\n",
    "        # Running the PhantomJS webdriver.\n",
    "        driver = webdriver.PhantomJS()\n",
    "        driver.set_window_size(1120, 550)\n",
    "    else:\n",
    "        # Defining the option to the Firefox webdriver.\n",
    "        options = Options()\n",
    "        Options.set_headless = True\n",
    "\n",
    "        # Running the Firefox webdriver.\n",
    "        driver = webdriver.Firefox(\n",
    "            executable_path = \"/home/breno/geckodriver/geckodriver\", options=options)\n",
    "\n",
    "    # Getting the web page.\n",
    "    driver.get(url)\n",
    "\n",
    "    # Setting the time of page refresh to 1 day (24 hours).\n",
    "    driver.execute_script(\"propriedadeTempoDoRefreshAutomatico = 86400000;\")\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(driver):\n",
    "    # Waiting for 10 seconds.\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Clicking the button \"ENTRAR\".\n",
    "    driver.find_element_by_id(\"barra-item-login\").click()\n",
    "\n",
    "    # Opening the iframe \"login\".\n",
    "    WebDriverWait(driver, 60).until(EC.frame_to_be_available_and_switch_to_it(\n",
    "        (By.ID, \"login-popin-iframe\")))\n",
    "\n",
    "    # Authenticating with user's account data.\n",
    "    username_field = driver.find_element_by_id(\"login\")\n",
    "    password_field = driver.find_element_by_id(\"password\")\n",
    "    username_field.send_keys(\">>> VALID USER/E-MAIL <<<\")\n",
    "    password_field.send_keys(\">>> YOUR PASSWORD <<<\")\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Waiting for 10 seconds.\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Returning the main window.\n",
    "    driver.switch_to.default_content()\n",
    "\n",
    "    # Waiting for 10 seconds.\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Forcing the time of page refresh to be 1 day (24 hours).\n",
    "    driver.execute_script(\"propriedadeTempoDoRefreshAutomatico = 86400000;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    # Getting Firefox webdriver.\n",
    "    driver = init_webdriver(url)\n",
    "\n",
    "    # Clicking the button \"PROSSEGUIR\".\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable(\n",
    "        (By.CSS_SELECTOR, \"button.cookie-banner-lgpd_accept-button\"))).click()\n",
    "\n",
    "    # Clicking the button \"CANCELAR\".\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable(\n",
    "        (By.ID, \"onesignal-slidedown-cancel-button\"))).click()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Waiting to load the news.\n",
    "            WebDriverWait(driver, 120).until(EC.visibility_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \".article-feed\")))\n",
    "\n",
    "            # Waiting to load the button \"Ver Mais\".\n",
    "            WebDriverWait(driver, 30).until(EC.element_to_be_clickable(\n",
    "                (By.CLASS_NAME, \"article-feed__more-button\")))\n",
    "\n",
    "            # Scrolling down to the button and clicking it.\n",
    "            button = driver.find_element_by_class_name(\"article-feed__more-button\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        except TimeoutException:\n",
    "            break\n",
    "        except StaleElementReferenceException:\n",
    "            break\n",
    "\n",
    "    # Extracting the news URLs.\n",
    "    html_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    links = [link[\"href\"] for link in html_soup.select(\"h1[class='article-feed-item__title'] > a\")]\n",
    "\n",
    "    # Closing the Firefox webdriver.\n",
    "    driver.quit()\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, links):\n",
    "    # Getting PhantomJS webdriver.\n",
    "    driver = init_webdriver(url, False)\n",
    "\n",
    "    # Authenticating the valid user.\n",
    "    authenticate(driver)\n",
    "\n",
    "    data = []\n",
    "    for idx, link in enumerate(links):\n",
    "        try:\n",
    "            # Getting the web page.\n",
    "            driver.get(link)\n",
    "\n",
    "            # Defining the scraper.\n",
    "            html_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            record = {}\n",
    "\n",
    "            # Title.\n",
    "            if html_soup.find(\"h1\", class_=\"article__title\"):\n",
    "                record[\"title\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"h1\", class_=\"article__title\").string).strip()\n",
    "            elif html_soup.select(\"div.head-materia > h1\"):\n",
    "                record[\"title\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.select(\"div.head-materia > h1\")[0].string).strip()\n",
    "\n",
    "            # Subtitle.\n",
    "            if html_soup.find(\"div\", class_=\"article__subtitle\"):\n",
    "                record[\"subtitle\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"div\", class_=\"article__subtitle\").string).strip()\n",
    "            elif html_soup.select(\"div.head-materia > h2\"):\n",
    "                record[\"subtitle\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.select(\"div.head-materia > h2\")[0].string).strip()\n",
    "\n",
    "            # Authors.\n",
    "            if html_soup.find(\"div\", class_=\"article__author\"):\n",
    "                record[\"author\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"div\", class_=\"article__author\").string).strip()\n",
    "            elif html_soup.find(\"span\", class_=\"autor\"):\n",
    "                record[\"author\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"span\", class_=\"autor\").string).strip()\n",
    "\n",
    "            # Date of Publication.\n",
    "            if html_soup.find(\"div\", class_=\"article__date\"):\n",
    "                record[\"date\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"div\", class_=\"article__date\").string).strip()\n",
    "            elif html_soup.find(\"div\", class_=\"meta-data\"):\n",
    "                record[\"date\"] = re.sub(r\"\\s+\", \" \",\n",
    "                    html_soup.find(\"div\", class_=\"meta-data\").text).strip()\n",
    "\n",
    "            # Full text.\n",
    "            if html_soup.select(\"main.main-content > p\"):\n",
    "                record[\"text\"] = re.sub(r\"\\s+\", \" \", \" \".join(\n",
    "                    [tag_p.text for tag_p in html_soup.select(\"main.main-content > p\")])).strip()\n",
    "            elif html_soup.find_all(\"div\", class_=\"capituloPage\"):\n",
    "                record[\"text\"] = re.sub(r\"\\s+\", \" \", \" \".join(\n",
    "                    [tag_p.text for tag_div in html_soup.find_all(\"div\", class_=\"capituloPage\")\n",
    "                        for tag_p in tag_div.find_all(\"p\", recursive=True)])).strip()\n",
    "\n",
    "            data.append(record)\n",
    "        except Exception as e:\n",
    "            print(idx)\n",
    "            raise e\n",
    "\n",
    "    # Closing the PhantomJS webdriver.\n",
    "    driver.quit()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the URL of target page.\n",
    "url = \"https://oglobo.globo.com/fato-ou-fake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collecting the news URLs.\n",
    "links = list(set(get_links(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printing the number of links collected.\n",
    "print(\"Number of links collected: {}.\".format(len(links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the backup of news URLs.\n",
    "with open(\"links_bkp.txt\", \"w\") as file:\n",
    "    file.writelines([link + \"\\n\" for link in links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collecting the data.\n",
    "data = get_data(url, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the number of records collected.\n",
    "print(\"Number of records collected: {}.\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Saving the data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to define the rumor's classification.\n",
    "def set_classification(title):\n",
    "    if \"#FAKE\" in title and \"#FATO\" not in title:\n",
    "        return 1\n",
    "    elif \"#FAKE\" not in title and \"#FATO\" in title:\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe object.\n",
    "df_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating new features.\n",
    "df_data[\"id\"] = df_data.index.values + 1\n",
    "df_data[\"link\"] = links\n",
    "df_data[\"classification\"] = df_data.title.apply(set_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of \"classification\" column.\n",
    "df_data.classification.loc[\n",
    "    df_data.classification.notnull()] = df_data.classification.loc[\n",
    "        df_data.classification.notnull()].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting the columns.\n",
    "df_data = df_data[[\"id\", \"link\", \"date\", \"title\", \"subtitle\", \"text\", \"author\", \"classification\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking the information about the dataset.\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the data to CSV file.\n",
    "df_data.to_csv(\"o_globo_fato_fake.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('estudos': conda)",
   "language": "python",
   "name": "python36864bitestudoscondaefdbdfb7fca04c4cbc7cf079ba261d35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}